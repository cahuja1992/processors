rnnlm {

  // all these files are under corpora/processors-dynet
  train {
    embed = "dynet/en/glove/glove.840B.300d.txt"
    docFreq = "dynet/en/docfreq/gigawordDocFreq.sorted.freq.txt"
    minWordFreq = 50

    learnedWordEmbeddingSize = 32
    taskTrain = "train2.txt"

    mandatoryWords = "dynet/en/train_unique_words.txt" // words coming from the tasks we are learning (e.g., NER, POS)
    minMandatoryWordFreq = 1

    c2i = "org/clulab/lm/c2i-en.txt"

    charEmbeddingSize = 32
    charRnnStateSize = 16
    wordRnnStateSize = 128

    doTrain = false
    train = "./1-billion-word-language-modeling-benchmark-r13output/conll2003-1billion-train.txt"
    dev = "./1-billion-word-language-modeling-benchmark-r13output/dev_small.txt"
    logCheckpoint = 10000
    saveCheckpoint = 100000
    lmLabelCount = 40000
    batchSize = 1

    model = "rnnlm"
  }
}